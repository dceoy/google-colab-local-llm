{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":482466,"status":"ok","timestamp":1711974584428,"user":{"displayName":"Daichi Narushima","userId":"05796984461948993007"},"user_tz":-540},"id":"vhP31Ov302kX","outputId":"d463e75f-b372-43d1-8a6a-219cc6486dd4"},"outputs":[{"name":"stdout","output_type":"stream","text":["+ head -20 /etc/lsb-release /etc/os-release\n","==\u003e /etc/lsb-release \u003c==\n","DISTRIB_ID=Ubuntu\n","DISTRIB_RELEASE=22.04\n","DISTRIB_CODENAME=jammy\n","DISTRIB_DESCRIPTION=\"Ubuntu 22.04.3 LTS\"\n","\n","==\u003e /etc/os-release \u003c==\n","PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\n","NAME=\"Ubuntu\"\n","VERSION_ID=\"22.04\"\n","VERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\n","VERSION_CODENAME=jammy\n","ID=ubuntu\n","ID_LIKE=debian\n","HOME_URL=\"https://www.ubuntu.com/\"\n","SUPPORT_URL=\"https://help.ubuntu.com/\"\n","BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n","PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n","UBUNTU_CODENAME=jammy\n","+ df -Th\n","Filesystem     Type     Size  Used Avail Use% Mounted on\n","overlay        overlay  202G   31G  171G  16% /\n","tmpfs          tmpfs     64M     0   64M   0% /dev\n","shm            tmpfs     41G     0   41G   0% /dev/shm\n","/dev/root      ext2     2.0G  1.1G  849M  57% /usr/sbin/docker-init\n","/dev/sda1      ext4     242G   79G  164G  33% /opt/bin/.nvidia\n","tmpfs          tmpfs     42G   48K   42G   1% /var/colab\n","tmpfs          tmpfs     42G     0   42G   0% /proc/acpi\n","tmpfs          tmpfs     42G     0   42G   0% /proc/scsi\n","tmpfs          tmpfs     42G     0   42G   0% /sys/firmware\n","+ free -g\n","               total        used        free      shared  buff/cache   available\n","Mem:              83           0          78           0           3          82\n","Swap:              0           0           0\n","+ nvidia-smi\n","Mon Apr  1 12:21:40 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","+ apt-get -y update -qq\n","+ apt-get -y upgrade -qq\n","Extracting templates from packages: 100%\n","Preconfiguring packages ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../base-files_12ubuntu4.6_amd64.deb ...\n","Unpacking base-files (12ubuntu4.6) over (12ubuntu4.4) ...\n","Setting up base-files (12ubuntu4.6) ...\n","Installing new version of config file /etc/issue ...\n","Installing new version of config file /etc/issue.net ...\n","Installing new version of config file /etc/lsb-release ...\n","Installing new version of config file /etc/update-motd.d/10-help-text ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../bash_5.1-6ubuntu1.1_amd64.deb ...\n","Unpacking bash (5.1-6ubuntu1.1) over (5.1-6ubuntu1) ...\n","Setting up bash (5.1-6ubuntu1.1) ...\n","update-alternatives: using /usr/share/man/man7/bash-builtins.7.gz to provide /usr/share/man/man7/builtins.7.gz (builtins.7.gz) in auto mode\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../bsdutils_1%3a2.37.2-4ubuntu3.3_amd64.deb ...\n","Unpacking bsdutils (1:2.37.2-4ubuntu3.3) over (1:2.37.2-4ubuntu3) ...\n","Setting up bsdutils (1:2.37.2-4ubuntu3.3) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../coreutils_8.32-4.1ubuntu1.2_amd64.deb ...\n","Unpacking coreutils (8.32-4.1ubuntu1.2) over (8.32-4.1ubuntu1) ...\n","Setting up coreutils (8.32-4.1ubuntu1.2) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libapt-pkg-dev_2.4.12_amd64.deb ...\n","Unpacking libapt-pkg-dev:amd64 (2.4.12) over (2.4.11) ...\n","Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n","Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n","Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libapt-pkg6.0_2.4.12_amd64.deb ...\n","Unpacking libapt-pkg6.0:amd64 (2.4.12) over (2.4.11) ...\n","Setting up libapt-pkg6.0:amd64 (2.4.12) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../tar_1.34+dfsg-1ubuntu0.1.22.04.2_amd64.deb ...\n","Unpacking tar (1.34+dfsg-1ubuntu0.1.22.04.2) over (1.34+dfsg-1ubuntu0.1.22.04.1) ...\n","Setting up tar (1.34+dfsg-1ubuntu0.1.22.04.2) ...\n","update-alternatives: warning: forcing reinstallation of alternative /usr/sbin/rmt-tar because link group rmt is broken\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../dpkg_1.21.1ubuntu2.3_amd64.deb ...\n","Unpacking dpkg (1.21.1ubuntu2.3) over (1.21.1ubuntu2.2) ...\n","Setting up dpkg (1.21.1ubuntu2.3) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../login_1%3a4.8.1-2ubuntu2.2_amd64.deb ...\n","Unpacking login (1:4.8.1-2ubuntu2.2) over (1:4.8.1-2ubuntu2.1) ...\n","Setting up login (1:4.8.1-2ubuntu2.2) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libperl5.34_5.34.0-3ubuntu1.3_amd64.deb ...\n","Unpacking libperl5.34:amd64 (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n","Preparing to unpack .../perl_5.34.0-3ubuntu1.3_amd64.deb ...\n","Unpacking perl (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n","Preparing to unpack .../perl-base_5.34.0-3ubuntu1.3_amd64.deb ...\n","Unpacking perl-base (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n","Setting up perl-base (5.34.0-3ubuntu1.3) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../perl-modules-5.34_5.34.0-3ubuntu1.3_all.deb ...\n","Unpacking perl-modules-5.34 (5.34.0-3ubuntu1.3) over (5.34.0-3ubuntu1.2) ...\n","Preparing to unpack .../util-linux_2.37.2-4ubuntu3.3_amd64.deb ...\n","Unpacking util-linux (2.37.2-4ubuntu3.3) over (2.37.2-4ubuntu3) ...\n","Setting up util-linux (2.37.2-4ubuntu3.3) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libc-bin_2.35-0ubuntu3.6_amd64.deb ...\n","Unpacking libc-bin (2.35-0ubuntu3.6) over (2.35-0ubuntu3.4) ...\n","Setting up libc-bin (2.35-0ubuntu3.6) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../archives/apt_2.4.12_amd64.deb ...\n","Unpacking apt (2.4.12) over (2.4.11) ...\n","Setting up apt (2.4.12) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../apt-utils_2.4.12_amd64.deb ...\n","Unpacking apt-utils (2.4.12) over (2.4.11) ...\n","Preparing to unpack .../libgnutls30_3.7.3-4ubuntu1.4_amd64.deb ...\n","Unpacking libgnutls30:amd64 (3.7.3-4ubuntu1.4) over (3.7.3-4ubuntu1.2) ...\n","Setting up libgnutls30:amd64 (3.7.3-4ubuntu1.4) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libpam0g_1.4.0-11ubuntu2.4_amd64.deb ...\n","Unpacking libpam0g:amd64 (1.4.0-11ubuntu2.4) over (1.4.0-11ubuntu2.3) ...\n","Setting up libpam0g:amd64 (1.4.0-11ubuntu2.4) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libpam-modules-bin_1.4.0-11ubuntu2.4_amd64.deb ...\n","Unpacking libpam-modules-bin (1.4.0-11ubuntu2.4) over (1.4.0-11ubuntu2.3) ...\n","Setting up libpam-modules-bin (1.4.0-11ubuntu2.4) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libpam-modules_1.4.0-11ubuntu2.4_amd64.deb ...\n","Unpacking libpam-modules:amd64 (1.4.0-11ubuntu2.4) over (1.4.0-11ubuntu2.3) ...\n","Setting up libpam-modules:amd64 (1.4.0-11ubuntu2.4) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../mount_2.37.2-4ubuntu3.3_amd64.deb ...\n","Unpacking mount (2.37.2-4ubuntu3.3) over (2.37.2-4ubuntu3) ...\n","Preparing to unpack .../libblkid1_2.37.2-4ubuntu3.3_amd64.deb ...\n","Unpacking libblkid1:amd64 (2.37.2-4ubuntu3.3) over (2.37.2-4ubuntu3) ...\n","Setting up libblkid1:amd64 (2.37.2-4ubuntu3.3) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libmount1_2.37.2-4ubuntu3.3_amd64.deb ...\n","Unpacking libmount1:amd64 (2.37.2-4ubuntu3.3) over (2.37.2-4ubuntu3) ...\n","Setting up libmount1:amd64 (2.37.2-4ubuntu3.3) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libpam-runtime_1.4.0-11ubuntu2.4_all.deb ...\n","Unpacking libpam-runtime (1.4.0-11ubuntu2.4) over (1.4.0-11ubuntu2.3) ...\n","Setting up libpam-runtime (1.4.0-11ubuntu2.4) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libsmartcols1_2.37.2-4ubuntu3.3_amd64.deb ...\n","Unpacking libsmartcols1:amd64 (2.37.2-4ubuntu3.3) over (2.37.2-4ubuntu3) ...\n","Setting up libsmartcols1:amd64 (2.37.2-4ubuntu3.3) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../passwd_1%3a4.8.1-2ubuntu2.2_amd64.deb ...\n","Unpacking passwd (1:4.8.1-2ubuntu2.2) over (1:4.8.1-2ubuntu2.1) ...\n","Setting up passwd (1:4.8.1-2ubuntu2.2) ...\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../00-libprocps8_2%3a3.3.17-6ubuntu2.1_amd64.deb ...\n","Unpacking libprocps8:amd64 (2:3.3.17-6ubuntu2.1) over (2:3.3.17-6ubuntu2) ...\n","Preparing to unpack .../01-procps_2%3a3.3.17-6ubuntu2.1_amd64.deb ...\n","Unpacking procps (2:3.3.17-6ubuntu2.1) over (2:3.3.17-6ubuntu2) ...\n","Preparing to unpack .../02-openssl_3.0.2-0ubuntu1.15_amd64.deb ...\n","Unpacking openssl (3.0.2-0ubuntu1.15) over (3.0.2-0ubuntu1.12) ...\n","Preparing to unpack .../03-libctf0_2.38-4ubuntu2.6_amd64.deb ...\n","Unpacking libctf0:amd64 (2.38-4ubuntu2.6) over (2.38-4ubuntu2.3) ...\n","Preparing to unpack .../04-libctf-nobfd0_2.38-4ubuntu2.6_amd64.deb ...\n","Unpacking libctf-nobfd0:amd64 (2.38-4ubuntu2.6) over (2.38-4ubuntu2.3) ...\n","Preparing to unpack .../05-binutils-x86-64-linux-gnu_2.38-4ubuntu2.6_amd64.deb ...\n","Unpacking binutils-x86-64-linux-gnu (2.38-4ubuntu2.6) over (2.38-4ubuntu2.3) ...\n","Preparing to unpack .../06-libbinutils_2.38-4ubuntu2.6_amd64.deb ...\n","Unpacking libbinutils:amd64 (2.38-4ubuntu2.6) over (2.38-4ubuntu2.3) ...\n","Preparing to unpack .../07-binutils_2.38-4ubuntu2.6_amd64.deb ...\n","Unpacking binutils (2.38-4ubuntu2.6) over (2.38-4ubuntu2.3) ...\n","Preparing to unpack .../08-binutils-common_2.38-4ubuntu2.6_amd64.deb ...\n","Unpacking binutils-common:amd64 (2.38-4ubuntu2.6) over (2.38-4ubuntu2.3) ...\n","Preparing to unpack .../09-cuda-compat-12-2_535.161.08-1_amd64.deb ...\n","Unpacking cuda-compat-12-2 (535.161.08-1) over (535.129.03-1) ...\n","Preparing to unpack .../10-cuda-keyring_1.1-1_all.deb ...\n","Unpacking cuda-keyring (1.1-1) over (1.0-1) ...\n","Preparing to unpack .../11-cuda-toolkit-12-config-common_12.4.99-1_all.deb ...\n","Unpacking cuda-toolkit-12-config-common (12.4.99-1) over (12.3.52-1) ...\n","Preparing to unpack .../12-cuda-toolkit-config-common_12.4.99-1_all.deb ...\n","Unpacking cuda-toolkit-config-common (12.4.99-1) over (12.3.52-1) ...\n","Preparing to unpack .../13-dpkg-dev_1.21.1ubuntu2.3_all.deb ...\n","Unpacking dpkg-dev (1.21.1ubuntu2.3) over (1.21.1ubuntu2.2) ...\n","Preparing to unpack .../14-libdpkg-perl_1.21.1ubuntu2.3_all.deb ...\n","Unpacking libdpkg-perl (1.21.1ubuntu2.3) over (1.21.1ubuntu2.2) ...\n","Preparing to unpack .../15-libldap-2.5-0_2.5.17+dfsg-0ubuntu0.22.04.1_amd64.deb ...\n","Unpacking libldap-2.5-0:amd64 (2.5.17+dfsg-0ubuntu0.22.04.1) over (2.5.16+dfsg-0ubuntu0.22.04.1) ...\n","Preparing to unpack .../16-linux-libc-dev_5.15.0-101.111_amd64.deb ...\n","Unpacking linux-libc-dev:amd64 (5.15.0-101.111) over (5.15.0-88.98) ...\n","Setting up cuda-toolkit-config-common (12.4.99-1) ...\n","Setting up cuda-compat-12-2 (535.161.08-1) ...\n","Setting up apt-utils (2.4.12) ...\n","Setting up binutils-common:amd64 (2.38-4ubuntu2.6) ...\n","Setting up linux-libc-dev:amd64 (5.15.0-101.111) ...\n","Setting up libctf-nobfd0:amd64 (2.38-4ubuntu2.6) ...\n","Setting up libapt-pkg-dev:amd64 (2.4.12) ...\n","Setting up perl-modules-5.34 (5.34.0-3ubuntu1.3) ...\n","Setting up libldap-2.5-0:amd64 (2.5.17+dfsg-0ubuntu0.22.04.1) ...\n","Setting up cuda-keyring (1.1-1) ...\n","Setting up mount (2.37.2-4ubuntu3.3) ...\n","Setting up libbinutils:amd64 (2.38-4ubuntu2.6) ...\n","Setting up openssl (3.0.2-0ubuntu1.15) ...\n","Setting up cuda-toolkit-12-config-common (12.4.99-1) ...\n","Setting up libprocps8:amd64 (2:3.3.17-6ubuntu2.1) ...\n","Setting up libctf0:amd64 (2.38-4ubuntu2.6) ...\n","Setting up libperl5.34:amd64 (5.34.0-3ubuntu1.3) ...\n","Setting up perl (5.34.0-3ubuntu1.3) ...\n","Setting up libdpkg-perl (1.21.1ubuntu2.3) ...\n","Setting up procps (2:3.3.17-6ubuntu2.1) ...\n","Setting up binutils-x86-64-linux-gnu (2.38-4ubuntu2.6) ...\n","Setting up binutils (2.38-4ubuntu2.6) ...\n","Setting up dpkg-dev (1.21.1ubuntu2.3) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.6) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","+ apt-get -y install -qq --no-install-recommends --no-install-suggests aria2\n","Selecting previously unselected package libc-ares2:amd64.\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n","Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n","Selecting previously unselected package libaria2-0:amd64.\n","Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n","Unpacking libaria2-0:amd64 (1.36.0-1) ...\n","Selecting previously unselected package aria2.\n","Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n","Unpacking aria2 (1.36.0-1) ...\n","Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n","Setting up libaria2-0:amd64 (1.36.0-1) ...\n","Setting up aria2 (1.36.0-1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.6) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","+ apt-get -y autoremove -qq\n","+ apt-get clean -qq\n","+ rm -rf /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy-backports_InRelease /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy-backports_main_binary-amd64_Packages.lz4 /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy-backports_universe_binary-amd64_Packages.lz4 /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy_InRelease /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy_main_binary-amd64_Packages.lz4 /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy_multiverse_binary-amd64_Packages.lz4 /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy_restricted_binary-amd64_Packages.lz4 /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy_universe_binary-amd64_Packages.lz4 /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy-updates_InRelease /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy-updates_main_binary-amd64_Packages.lz4 /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy-updates_multiverse_binary-amd64_Packages.lz4 /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy-updates_restricted_binary-amd64_Packages.lz4 /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_jammy-updates_universe_binary-amd64_Packages.lz4 /var/lib/apt/lists/auxfiles /var/lib/apt/lists/cloud.r-project.org_bin_linux_ubuntu_jammy-cran40_InRelease /var/lib/apt/lists/cloud.r-project.org_bin_linux_ubuntu_jammy-cran40_Packages.lz4 /var/lib/apt/lists/developer.download.nvidia.com_compute_cuda_repos_ubuntu2204_x86%5f64_InRelease /var/lib/apt/lists/developer.download.nvidia.com_compute_cuda_repos_ubuntu2204_x86%5f64_Packages.lz4 /var/lib/apt/lists/lock /var/lib/apt/lists/partial /var/lib/apt/lists/ppa.launchpadcontent.net_c2d4u.team_c2d4u4.0+_ubuntu_dists_jammy_InRelease /var/lib/apt/lists/ppa.launchpadcontent.net_c2d4u.team_c2d4u4.0+_ubuntu_dists_jammy_main_binary-amd64_Packages.lz4 /var/lib/apt/lists/ppa.launchpadcontent.net_c2d4u.team_c2d4u4.0+_ubuntu_dists_jammy_main_source_Sources.lz4 /var/lib/apt/lists/ppa.launchpadcontent.net_deadsnakes_ppa_ubuntu_dists_jammy_InRelease /var/lib/apt/lists/ppa.launchpadcontent.net_deadsnakes_ppa_ubuntu_dists_jammy_main_binary-amd64_Packages.lz4 /var/lib/apt/lists/ppa.launchpadcontent.net_graphics-drivers_ppa_ubuntu_dists_jammy_InRelease /var/lib/apt/lists/ppa.launchpadcontent.net_graphics-drivers_ppa_ubuntu_dists_jammy_main_binary-amd64_Packages.lz4 /var/lib/apt/lists/ppa.launchpadcontent.net_ubuntugis_ppa_ubuntu_dists_jammy_InRelease /var/lib/apt/lists/ppa.launchpadcontent.net_ubuntugis_ppa_ubuntu_dists_jammy_main_binary-amd64_Packages.lz4 /var/lib/apt/lists/security.ubuntu.com_ubuntu_dists_jammy-security_InRelease /var/lib/apt/lists/security.ubuntu.com_ubuntu_dists_jammy-security_main_binary-amd64_Packages.lz4 /var/lib/apt/lists/security.ubuntu.com_ubuntu_dists_jammy-security_multiverse_binary-amd64_Packages.lz4 /var/lib/apt/lists/security.ubuntu.com_ubuntu_dists_jammy-security_restricted_binary-amd64_Packages.lz4 /var/lib/apt/lists/security.ubuntu.com_ubuntu_dists_jammy-security_universe_binary-amd64_Packages.lz4\n","+ curl -sSLO https://raw.githubusercontent.com/dceoy/print-github-tags/master/print-github-tags\n","+ chmod +x print-github-tags\n","+ [[ ! -d text-generation-webui ]]\n","+ ./print-github-tags --release --latest --tar oobabooga/text-generation-webui\n","+ xargs -t curl -sSL -o text-generation-webui.tar.gz\n","curl -sSL -o text-generation-webui.tar.gz https://github.com/oobabooga/text-generation-webui/archive/snapshot-2024-03-31.tar.gz\n","+ tar xf text-generation-webui.tar.gz\n","+ rm -f text-generation-webui.tar.gz\n","+ mv text-generation-webui-snapshot-2024-03-31 text-generation-webui\n","+ pip install -q --no-cache-dir -r text-generation-webui/requirements.txt\n","+ LLM_URLS=('https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf' 'https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf' 'https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/resolve/main/llama-2-70b-chat.Q5_K_M.gguf' 'https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf' 'https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf/resolve/main/ELYZA-japanese-Llama-2-13b-fast-instruct-q8_0.gguf' 'https://huggingface.co/mmnga/RakutenAI-7B-chat-gguf/resolve/main/RakutenAI-7B-chat-q8_0.gguf')\n","+ for u in \"${LLM_URLS[@]}\"\n","++ basename https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf\n","+ aria2c --dir text-generation-webui/models --console-log-level=warn -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf -o llama-2-7b-chat.Q5_K_M.gguf\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m270.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 MB\u001b[0m \u001b[31m264.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m278.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m279.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 MB\u001b[0m \u001b[31m298.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.5/739.5 kB\u001b[0m \u001b[31m340.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m306.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m252.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m294.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m171.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m209.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m304.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m282.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m310.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m303.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.1/407.1 kB\u001b[0m \u001b[31m334.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m298.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m255.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m293.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m329.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m345.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 kB\u001b[0m \u001b[31m268.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m334.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m285.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","dc2e01|\u001b[1;32mOK\u001b[0m  |   244MiB/s|text-generation-webui/models/llama-2-7b-chat.Q5_K_M.gguf\n","\n","Status Legend:\n","(OK):download completed.\n","+ for u in \"${LLM_URLS[@]}\"\n","++ basename https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf\n","+ aria2c --dir text-generation-webui/models --console-log-level=warn -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf -o llama-2-13b-chat.Q5_K_M.gguf\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.4/312.4 kB\u001b[0m \u001b[31m329.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m284.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m238.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m308.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m241.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m325.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m303.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m334.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m312.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m288.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m253.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m316.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 MB\u001b[0m \u001b[31m265.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m213.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m246.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.4/33.4 MB\u001b[0m \u001b[31m247.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m256.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m290.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m278.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m164.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m304.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.9/264.9 kB\u001b[0m \u001b[31m297.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m216.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m259.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m300.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m153.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m237.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m224.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m286.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m320.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m284.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m136.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m152.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m308.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m276.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m279.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m298.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m273.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m278.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m187.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m248.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m315.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[0m  Building wheel for hqq (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[0m  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[0m  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[0m  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","d6848e|\u001b[1;32mOK\u001b[0m  |   245MiB/s|text-generation-webui/models/llama-2-13b-chat.Q5_K_M.gguf\n","\n","Status Legend:\n","(OK):download completed.\n","+ for u in \"${LLM_URLS[@]}\"\n","++ basename https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/resolve/main/llama-2-70b-chat.Q5_K_M.gguf\n","+ aria2c --dir text-generation-webui/models --console-log-level=warn -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/resolve/main/llama-2-70b-chat.Q5_K_M.gguf -o llama-2-70b-chat.Q5_K_M.gguf\n"," *** Download Progress Summary as of Mon Apr  1 12:24:50 2024 *** \n","=\n","[#5bedbb 18GiB/45GiB(41%) CN:16 DL:162MiB ETA:2m47s]\n","FILE: text-generation-webui/models/llama-2-70b-chat.Q5_K_M.gguf\n","-\n","\n"," *** Download Progress Summary as of Mon Apr  1 12:25:50 2024 *** \n","=\n","[#5bedbb 36GiB/45GiB(80%) CN:16 DL:216MiB ETA:42s]\n","FILE: text-generation-webui/models/llama-2-70b-chat.Q5_K_M.gguf\n","-\n","\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","5bedbb|\u001b[1;32mOK\u001b[0m  |   289MiB/s|text-generation-webui/models/llama-2-70b-chat.Q5_K_M.gguf\n","\n","Status Legend:\n","(OK):download completed.\n","+ for u in \"${LLM_URLS[@]}\"\n","++ basename https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\n","+ aria2c --dir text-generation-webui/models --console-log-level=warn -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf -o mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\n"," *** Download Progress Summary as of Mon Apr  1 12:27:30 2024 *** \n","=\n","[#f04245 19GiB/30GiB(63%) CN:16 DL:227MiB ETA:49s]\n","FILE: text-generation-webui/models/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\n","-\n","\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","f04245|\u001b[1;32mOK\u001b[0m  |   322MiB/s|text-generation-webui/models/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\n","\n","Status Legend:\n","(OK):download completed.\n","+ for u in \"${LLM_URLS[@]}\"\n","++ basename https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf/resolve/main/ELYZA-japanese-Llama-2-13b-fast-instruct-q8_0.gguf\n","+ aria2c --dir text-generation-webui/models --console-log-level=warn -x 16 -s 16 -k 1M https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf/resolve/main/ELYZA-japanese-Llama-2-13b-fast-instruct-q8_0.gguf -o ELYZA-japanese-Llama-2-13b-fast-instruct-q8_0.gguf\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","8b6cff|\u001b[1;32mOK\u001b[0m  |   292MiB/s|text-generation-webui/models/ELYZA-japanese-Llama-2-13b-fast-instruct-q8_0.gguf\n","\n","Status Legend:\n","(OK):download completed.\n","+ for u in \"${LLM_URLS[@]}\"\n","++ basename https://huggingface.co/mmnga/RakutenAI-7B-chat-gguf/resolve/main/RakutenAI-7B-chat-q8_0.gguf\n","+ aria2c --dir text-generation-webui/models --console-log-level=warn -x 16 -s 16 -k 1M https://huggingface.co/mmnga/RakutenAI-7B-chat-gguf/resolve/main/RakutenAI-7B-chat-q8_0.gguf -o RakutenAI-7B-chat-q8_0.gguf\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","5acfc0|\u001b[1;32mOK\u001b[0m  |   262MiB/s|text-generation-webui/models/RakutenAI-7B-chat-q8_0.gguf\n","\n","Status Legend:\n","(OK):download completed.\n","+ wait\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","imageio 2.31.6 requires pillow\u003c10.1.0,\u003e=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m+ python text-generation-webui/server.py --auto-devices --chat --share --model-dir text-generation-webui/models\n","\u001b[2;36m12:29:41-622865\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting Text generation web UI                                            \n","\u001b[30m╭─\u001b[0m\u001b[30m──────────────────────────────\u001b[0m\u001b[30m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[30m \u001b[0m\u001b[30m───────────────────────────────\u001b[0m\u001b[30m─╮\u001b[0m\n","\u001b[30m│\u001b[0m \u001b[2;33m/content/text-generation-webui/\u001b[0m\u001b[1;33mserver.py\u001b[0m:\u001b[94m255\u001b[0m in \u001b[92m\u003cmodule\u003e\u001b[0m                                         \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m                                                                                                  \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m   \u001b[2m254 \u001b[0m        \u001b[2m# Launch the web UI\u001b[0m                                                                \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m \u001b[31m❱ \u001b[0m255         create_interface()                                                                 \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m   \u001b[2m256 \u001b[0m        \u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                        \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m                                                                                                  \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m \u001b[2;33m/content/text-generation-webui/\u001b[0m\u001b[1;33mserver.py\u001b[0m:\u001b[94m129\u001b[0m in \u001b[92mcreate_interface\u001b[0m                                 \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m                                                                                                  \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m   \u001b[2m128 \u001b[0m        \u001b[2m# Text Generation tab\u001b[0m                                                              \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m \u001b[31m❱ \u001b[0m129         ui_chat.create_ui()                                                                \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m   \u001b[2m130 \u001b[0m        ui_default.create_ui()                                                             \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m                                                                                                  \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m \u001b[2;33m/content/text-generation-webui/modules/\u001b[0m\u001b[1;33mui_chat.py\u001b[0m:\u001b[94m27\u001b[0m in \u001b[92mcreate_ui\u001b[0m                                \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m                                                                                                  \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m   \u001b[2m 26 \u001b[0m            \u001b[94mwith\u001b[0m gr.Column(elem_id=\u001b[33m'\u001b[0m\u001b[33mchat-col\u001b[0m\u001b[33m'\u001b[0m):                                            \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m \u001b[31m❱ \u001b[0m 27                 shared.gradio[\u001b[33m'\u001b[0m\u001b[33mdisplay\u001b[0m\u001b[33m'\u001b[0m] = gr.HTML(value=chat_html_wrapper({\u001b[33m'\u001b[0m\u001b[33minternal\u001b[0m\u001b[33m'\u001b[0m:    \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m   \u001b[2m 28 \u001b[0m                                                                                           \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m                                                                                                  \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m \u001b[2;33m/content/text-generation-webui/modules/\u001b[0m\u001b[1;33mhtml_generator.py\u001b[0m:\u001b[94m322\u001b[0m in \u001b[92mchat_html_wrapper\u001b[0m                \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m                                                                                                  \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m   \u001b[2m321 \u001b[0m    \u001b[94melse\u001b[0m:                                                                                  \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m \u001b[31m❱ \u001b[0m322         \u001b[94mreturn\u001b[0m generate_cai_chat_html(history[\u001b[33m'\u001b[0m\u001b[33mvisible\u001b[0m\u001b[33m'\u001b[0m], name1, name2, style, character   \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m   \u001b[2m323 \u001b[0m                                                                                           \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m                                                                                                  \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m \u001b[2;33m/content/text-generation-webui/modules/\u001b[0m\u001b[1;33mhtml_generator.py\u001b[0m:\u001b[94m239\u001b[0m in \u001b[92mgenerate_cai_chat_html\u001b[0m           \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m                                                                                                  \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m   \u001b[2m238 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mgenerate_cai_chat_html\u001b[0m(history, name1, name2, style, character, reset_cache=\u001b[94mFalse\u001b[0m):    \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m \u001b[31m❱ \u001b[0m239     output = \u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\u003cstyle\u003e\u001b[0m\u001b[33m{\u001b[0mchat_styles[style]\u001b[33m}\u001b[0m\u001b[33m\u003c/style\u003e\u003cdiv class=\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mchat\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m id=\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mchat\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\u003e\u003cdiv class\u001b[0m   \u001b[30m│\u001b[0m\n","\u001b[30m│\u001b[0m   \u001b[2m240 \u001b[0m                                                                                           \u001b[30m│\u001b[0m\n","\u001b[30m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mKeyError: \u001b[0m\u001b[32m'cai-chat'\u001b[0m\n"]}],"source":["!curl -sSL https://raw.githubusercontent.com/dceoy/google-colab-local-llm/main/install-text-generation-webui.sh | bash"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Kgi90D6tpdZ3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2;36m13:03:03-552005\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting Text generation web UI                                            \n","\u001b[2;36m13:03:03-556813\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading the extension \u001b[32m\"gallery\"\u001b[0m                                            \n","\n","Running on local URL:  http://127.0.0.1:7860\n","\n","Running on public URL: https://91bc9ba9fa2da4b497.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","\u001b[2;36m13:04:00-706709\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"llama-2-70b-chat.Q5_K_M.gguf\"\u001b[0m                                     \n","\u001b[2;36m13:04:00-860631\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m llama.cpp weights detected: \u001b[32m\"models/llama-2-70b-chat.Q5_K_M.gguf\"\u001b[0m          \n","ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   yes\n","ggml_init_cublas: CUDA_USE_TENSOR_CORES: no\n","ggml_init_cublas: found 1 CUDA devices:\n","  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n","llama_model_loader: loaded meta data with 19 key-value pairs and 723 tensors from models/llama-2-70b-chat.Q5_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                          general.file_type u32              = 17\n","llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"\u003cunk\u003e\", \"\u003cs\u003e\", \"\u003c/s\u003e\", \"\u003c0x00\u003e\", \"\u003c...\n","llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:  161 tensors\n","llama_model_loader: - type q5_K:  481 tensors\n","llama_model_loader: - type q6_K:   81 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 8192\n","llm_load_print_meta: n_head           = 64\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 80\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 8\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 28672\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 70B\n","llm_load_print_meta: model ftype      = Q5_K - Medium\n","llm_load_print_meta: model params     = 68.98 B\n","llm_load_print_meta: model size       = 45.40 GiB (5.65 BPW) \n","llm_load_print_meta: general.name     = LLaMA v2\n","llm_load_print_meta: BOS token        = 1 '\u003cs\u003e'\n","llm_load_print_meta: EOS token        = 2 '\u003c/s\u003e'\n","llm_load_print_meta: UNK token        = 0 '\u003cunk\u003e'\n","llm_load_print_meta: LF token         = 13 '\u003c0x0A\u003e'\n","llm_load_tensors: ggml ctx size =    0.55 MiB\n","ggml_backend_cuda_buffer_type_alloc_buffer: allocating 46117.50 MiB on device 0: cudaMalloc failed: out of memory\n","llama_model_load: error loading model: failed to allocate buffer\n","llama_load_model_from_file: failed to load model\n","\u001b[2;36m13:04:04-533706\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mERROR   \u001b[0m Failed to load the model.                                                  \n","Traceback (most recent call last):\n","  File \"/content/text-generation-webui/modules/ui_model_menu.py\", line 245, in load_model_wrapper\n","    shared.model, shared.tokenizer = load_model(selected_model, loader)\n","  File \"/content/text-generation-webui/modules/models.py\", line 87, in load_model\n","    output = load_func_map[loader](model_name)\n","  File \"/content/text-generation-webui/modules/models.py\", line 250, in llamacpp_loader\n","    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n","  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 102, in from_pretrained\n","    result.model = Llama(**params)\n","  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py\", line 311, in __init__\n","    self._model = _LlamaModel(\n","  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/_internals.py\", line 55, in __init__\n","    raise ValueError(f\"Failed to load model from file: {path_model}\")\n","ValueError: Failed to load model from file: models/llama-2-70b-chat.Q5_K_M.gguf\n","\n","Exception ignored in: \u003cfunction LlamaCppModel.__del__ at 0x790e984d8c10\u003e\n","Traceback (most recent call last):\n","  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 58, in __del__\n","    del self.model\n","AttributeError: model\n","\u001b[2;36m13:04:29-988895\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"llama-2-70b-chat.Q5_K_M.gguf\"\u001b[0m                                     \n","\u001b[2;36m13:04:30-053421\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m llama.cpp weights detected: \u001b[32m\"models/llama-2-70b-chat.Q5_K_M.gguf\"\u001b[0m          \n","llama_model_loader: loaded meta data with 19 key-value pairs and 723 tensors from models/llama-2-70b-chat.Q5_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                          general.file_type u32              = 17\n","llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"\u003cunk\u003e\", \"\u003cs\u003e\", \"\u003c/s\u003e\", \"\u003c0x00\u003e\", \"\u003c...\n","llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:  161 tensors\n","llama_model_loader: - type q5_K:  481 tensors\n","llama_model_loader: - type q6_K:   81 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 8192\n","llm_load_print_meta: n_head           = 64\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 80\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 8\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 28672\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 70B\n","llm_load_print_meta: model ftype      = Q5_K - Medium\n","llm_load_print_meta: model params     = 68.98 B\n","llm_load_print_meta: model size       = 45.40 GiB (5.65 BPW) \n","llm_load_print_meta: general.name     = LLaMA v2\n","llm_load_print_meta: BOS token        = 1 '\u003cs\u003e'\n","llm_load_print_meta: EOS token        = 2 '\u003c/s\u003e'\n","llm_load_print_meta: UNK token        = 0 '\u003cunk\u003e'\n","llm_load_print_meta: LF token         = 13 '\u003c0x0A\u003e'\n","llm_load_tensors: ggml ctx size =    0.55 MiB\n","ggml_backend_cuda_buffer_type_alloc_buffer: allocating 46117.50 MiB on device 0: cudaMalloc failed: out of memory\n","llama_model_load: error loading model: failed to allocate buffer\n","llama_load_model_from_file: failed to load model\n","\u001b[2;36m13:04:33-635544\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mERROR   \u001b[0m Failed to load the model.                                                  \n","Traceback (most recent call last):\n","  File \"/content/text-generation-webui/modules/ui_model_menu.py\", line 245, in load_model_wrapper\n","    shared.model, shared.tokenizer = load_model(selected_model, loader)\n","  File \"/content/text-generation-webui/modules/models.py\", line 87, in load_model\n","    output = load_func_map[loader](model_name)\n","  File \"/content/text-generation-webui/modules/models.py\", line 250, in llamacpp_loader\n","    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n","  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 102, in from_pretrained\n","    result.model = Llama(**params)\n","  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py\", line 311, in __init__\n","    self._model = _LlamaModel(\n","  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/_internals.py\", line 55, in __init__\n","    raise ValueError(f\"Failed to load model from file: {path_model}\")\n","ValueError: Failed to load model from file: models/llama-2-70b-chat.Q5_K_M.gguf\n","\n","Exception ignored in: \u003cfunction LlamaCppModel.__del__ at 0x790e984d8c10\u003e\n","Traceback (most recent call last):\n","  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 58, in __del__\n","    del self.model\n","AttributeError: model\n","\u001b[2;36m13:04:50-365653\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"llama-2-70b-chat.Q5_K_M.gguf\"\u001b[0m                                     \n","\u001b[2;36m13:04:50-430084\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m llama.cpp weights detected: \u001b[32m\"models/llama-2-70b-chat.Q5_K_M.gguf\"\u001b[0m          \n","llama_model_loader: loaded meta data with 19 key-value pairs and 723 tensors from models/llama-2-70b-chat.Q5_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                          general.file_type u32              = 17\n","llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"\u003cunk\u003e\", \"\u003cs\u003e\", \"\u003c/s\u003e\", \"\u003c0x00\u003e\", \"\u003c...\n","llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:  161 tensors\n","llama_model_loader: - type q5_K:  481 tensors\n","llama_model_loader: - type q6_K:   81 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 8192\n","llm_load_print_meta: n_head           = 64\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 80\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 8\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 28672\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 70B\n","llm_load_print_meta: model ftype      = Q5_K - Medium\n","llm_load_print_meta: model params     = 68.98 B\n","llm_load_print_meta: model size       = 45.40 GiB (5.65 BPW) \n","llm_load_print_meta: general.name     = LLaMA v2\n","llm_load_print_meta: BOS token        = 1 '\u003cs\u003e'\n","llm_load_print_meta: EOS token        = 2 '\u003c/s\u003e'\n","llm_load_print_meta: UNK token        = 0 '\u003cunk\u003e'\n","llm_load_print_meta: LF token         = 13 '\u003c0x0A\u003e'\n","llm_load_tensors: ggml ctx size =    0.55 MiB\n","ggml_backend_cuda_buffer_type_alloc_buffer: allocating 46117.50 MiB on device 0: cudaMalloc failed: out of memory\n","llama_model_load: error loading model: failed to allocate buffer\n","llama_load_model_from_file: failed to load model\n","\u001b[2;36m13:04:53-996953\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mERROR   \u001b[0m Failed to load the model.                                                  \n","Traceback (most recent call last):\n","  File \"/content/text-generation-webui/modules/ui_model_menu.py\", line 245, in load_model_wrapper\n","    shared.model, shared.tokenizer = load_model(selected_model, loader)\n","  File \"/content/text-generation-webui/modules/models.py\", line 87, in load_model\n","    output = load_func_map[loader](model_name)\n","  File \"/content/text-generation-webui/modules/models.py\", line 250, in llamacpp_loader\n","    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n","  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 102, in from_pretrained\n","    result.model = Llama(**params)\n","  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py\", line 311, in __init__\n","    self._model = _LlamaModel(\n","  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/_internals.py\", line 55, in __init__\n","    raise ValueError(f\"Failed to load model from file: {path_model}\")\n","ValueError: Failed to load model from file: models/llama-2-70b-chat.Q5_K_M.gguf\n","\n","Exception ignored in: \u003cfunction LlamaCppModel.__del__ at 0x790e984d8c10\u003e\n","Traceback (most recent call last):\n","  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 58, in __del__\n","    del self.model\n","AttributeError: model\n","\u001b[2;36m13:05:12-409933\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"llama-2-70b-chat.Q5_K_M.gguf\"\u001b[0m                                     \n","\u001b[2;36m13:05:12-473827\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m llama.cpp weights detected: \u001b[32m\"models/llama-2-70b-chat.Q5_K_M.gguf\"\u001b[0m          \n","llama_model_loader: loaded meta data with 19 key-value pairs and 723 tensors from models/llama-2-70b-chat.Q5_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                          general.file_type u32              = 17\n","llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"\u003cunk\u003e\", \"\u003cs\u003e\", \"\u003c/s\u003e\", \"\u003c0x00\u003e\", \"\u003c...\n","llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:  161 tensors\n","llama_model_loader: - type q5_K:  481 tensors\n","llama_model_loader: - type q6_K:   81 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 8192\n","llm_load_print_meta: n_head           = 64\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 80\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 8\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 28672\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 70B\n","llm_load_print_meta: model ftype      = Q5_K - Medium\n","llm_load_print_meta: model params     = 68.98 B\n","llm_load_print_meta: model size       = 45.40 GiB (5.65 BPW) \n","llm_load_print_meta: general.name     = LLaMA v2\n","llm_load_print_meta: BOS token        = 1 '\u003cs\u003e'\n","llm_load_print_meta: EOS token        = 2 '\u003c/s\u003e'\n","llm_load_print_meta: UNK token        = 0 '\u003cunk\u003e'\n","llm_load_print_meta: LF token         = 13 '\u003c0x0A\u003e'\n","llm_load_tensors: ggml ctx size =    0.55 MiB\n","llm_load_tensors: offloading 48 repeating layers to GPU\n","llm_load_tensors: offloaded 48/81 layers to GPU\n","llm_load_tensors:        CPU buffer size = 46494.48 MiB\n","llm_load_tensors:      CUDA0 buffer size = 27639.69 MiB\n","....................................................................................................\n","llama_new_context_with_model: n_ctx      = 1024\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:  CUDA_Host KV buffer size =   128.00 MiB\n","llama_kv_cache_init:      CUDA0 KV buffer size =   192.00 MiB\n","llama_new_context_with_model: KV self size  =  320.00 MiB, K (f16):  160.00 MiB, V (f16):  160.00 MiB\n","llama_new_context_with_model:  CUDA_Host input buffer size   =     0.04 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =     0.38 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =     0.41 MiB\n","llama_new_context_with_model: graph splits (measure): 3\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n","Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n","Using fallback chat format: None\n","\u001b[2;36m13:05:21-160860\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LOADER: \u001b[32m\"llama.cpp\"\u001b[0m                                                        \n","\u001b[2;36m13:05:21-162499\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRUNCATION LENGTH: \u001b[1;36m1024\u001b[0m                                                    \n","\u001b[2;36m13:05:21-163580\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INSTRUCTION TEMPLATE: \u001b[32m\"Llama-v2\"\u001b[0m                                           \n","\u001b[2;36m13:05:21-164719\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded the model in \u001b[1;36m8.75\u001b[0m seconds.                                          \n","Output generated in 64.31 seconds (0.37 tokens/s, 24 tokens, context 73, seed 137157554)\n","Llama.generate: prefix-match hit\n","Output generated in 23.05 seconds (1.04 tokens/s, 24 tokens, context 108, seed 640180923)\n","Llama.generate: prefix-match hit\n","Output generated in 28.74 seconds (1.01 tokens/s, 29 tokens, context 146, seed 1414191441)\n","Llama.generate: prefix-match hit\n","Output generated in 66.81 seconds (1.17 tokens/s, 78 tokens, context 197, seed 1607945306)\n","Llama.generate: prefix-match hit\n","Output generated in 111.15 seconds (1.33 tokens/s, 148 tokens, context 293, seed 424274761)\n","Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =     803.52 ms\n","llama_print_timings:      sample time =      37.02 ms /    67 runs   (    0.55 ms per token,  1809.73 tokens per second)\n","llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n","llama_print_timings:        eval time =   67850.08 ms /   101 runs   (  671.78 ms per token,     1.49 tokens per second)\n","llama_print_timings:       total time =   68234.86 ms /   102 tokens\n","Output generated in 68.77 seconds (0.96 tokens/s, 66 tokens, context 476, seed 529818537)\n","Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =     803.52 ms\n","llama_print_timings:      sample time =      19.73 ms /    35 runs   (    0.56 ms per token,  1773.59 tokens per second)\n","llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n","llama_print_timings:        eval time =  330103.82 ms /   492 runs   (  670.94 ms per token,     1.49 tokens per second)\n","llama_print_timings:       total time =  331417.85 ms /   493 tokens\n","Output generated in 331.95 seconds (0.10 tokens/s, 34 tokens, context 510, seed 876086732)\n"]}],"source":["!cd text-generation-webui \u0026\u0026 python server.py --auto-devices --chat --share --model-dir models"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPSj3NdqCVJWPFAtE9PxKry","gpuType":"A100","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}